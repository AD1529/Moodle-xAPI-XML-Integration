{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from fontTools.ttLib.tables.otTraverse import dfs_base_table\n",
    "\n",
    "import src.MoodleLogsAlgorithms.data_preprocessing as pp\n",
    "import src.MoodleLogsAlgorithms.function_utils as fu\n",
    "import src.DataExploration.data_description as de\n",
    "import src.MoodleLogsAlgorithms.data_cleaning as cl\n",
    "import src.MoodleLogsAlgorithms.data_integration as di\n",
    "import src.MoodleLogsAlgorithms.data_selection as ds\n",
    "\n",
    "PATH_JSON = Path(\"../Datasets/JSON_files/\")\n",
    "PATH_PICKLE = Path(\"../Datasets/xAPI_logs/PICKLE_files/\")\n",
    "PATH_PREPROC = Path(\"../Datasets/xAPI_logs/PREPROCESSED_files/\")\n",
    "COURSES = [125, 141, 153, 313, 1539, 2961, 3135, 3559, 3789, 3791]\n",
    "\n",
    "COURSES_NAMES = ['Modélisation Numérique en Physique - S1-23',  # 125\n",
    "                 'Short talks - S2-23',  # 141\n",
    "                 'Informatique pour Géosciences 1 - S1-23',  # 153\n",
    "                 'Minéralogie - S1-23',  # 313\n",
    "                 'LU2IN011 - Représentation et méthodes numériques - S1-23',  # 1539\n",
    "                 'Diversité des Interactions Marines - S1-23',  # 2961\n",
    "                 'Label vert 2 - S1-23',  # 3135\n",
    "                 'Modélisation Numérique en Physique - S2-23',  # 3559\n",
    "                 'Introduction aux enjeux environnementaux IMTT-ENV-S2-23',  # 3789\n",
    "                 'IMTT-GES-S2-23'  # 3791\n",
    "                 ]\n",
    "\n",
    "for course in COURSES:\n",
    "    # ----------------------------\n",
    "    # DATA PREPROCESSING\n",
    "    # ----------------------------\n",
    "    pp.read_json_file(course=course, path_json=PATH_JSON, path_pickle=PATH_PICKLE)\n",
    "    print('Course: ', course)\n",
    "    df = pd.read_pickle(f\"{PATH_PICKLE}/cours_{course}.pkl\")\n",
    "    # fix mistakes\n",
    "    df = fu.patch(df, f\"{PATH_PICKLE}/prior_statements.pkl\")  # TODO: remove on GitHub\n",
    "    # preprocess dataframe data\n",
    "    df = pp.filter_and_rename_columns(df)\n",
    "    df = pp.merge_duplicates_missing_values(df)\n",
    "    df = pp.add_component_event_name(df)\n",
    "    df = pp.extract_ids(df)\n",
    "    df = pp.add_course_area(df)\n",
    "    df = pp.redefine_component(df)\n",
    "    df = pp.redefine_event_name(df)\n",
    "    # fix mistakes\n",
    "    df = fu.patch_modified_names(df)  # TODO: remove on GitHub\n",
    "\n",
    "    # preprocess user roles\n",
    "    role_table = pp.get_role_table(df, course)\n",
    "    # check for fake students\n",
    "    students_to_remove = pp.detect_potential_fake_students(df, course)\n",
    "    role_table = pp.remove_fake_students(role_table, students_to_remove)\n",
    "    # assign roles\n",
    "    df = pp.assign_roles(role_table, df, course)\n",
    "\n",
    "    # ----------------------------\n",
    "    # DATA INTEGRATION\n",
    "    # ----------------------------\n",
    "    # ICAP\n",
    "    df = di.integrate_icap_framework(df)\n",
    "    # groups\n",
    "    group_table = di.get_group_table(df, course)\n",
    "    df = di.assign_groups(group_table, df, course)\n",
    "    # activity status\n",
    "    df = di.add_activity_status(df)\n",
    "\n",
    "    # ----------------------------\n",
    "    # DATA SELECTION\n",
    "    # ----------------------------\n",
    "    # filter data based on the timestamp\n",
    "    df = ds.filter_semester_data(df, course)\n",
    "\n",
    "    # ----------------------------\n",
    "    # DATA CLEANING\n",
    "    # ----------------------------\n",
    "    # clean the dataset from worthless events\n",
    "    df = cl.clean_events(df)\n",
    "\n",
    "    PATH_PREPROC = Path(\"../Datasets/xAPI_logs/PREPROCESSED_files\")\n",
    "    # save the dataset for the analysis\n",
    "    df.to_pickle(f\"{PATH_PREPROC}/cours_{course}.pkl\")\n"
   ],
   "id": "18e0b2bb542d66ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9c0ff2ef6718a7c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7676d999d40a2946"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "82b2cde4a087ce4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c92c4354da2beb97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b8c2ffab434fe62b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "860b8d05704741ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4094cc622c263c32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "25ab81d3fae69fdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import json\n",
    "\n",
    "file_path = '/Users/rotelli/PycharmProjects/analyse-de-cours-moodle/src/Datasets/xAPI_logs/JSON_files/prior_statements.json'\n",
    "with open(file_path, mode='r') as file:\n",
    "    json_file = json.load(file)\n",
    "\n",
    "# normalise the JSON file\n",
    "df = fu.normalise_json(json_file)\n",
    "# save the normalised file as pickle\n",
    "converted_file_path = '/Users/rotelli/PycharmProjects/analyse-de-cours-moodle/src/Datasets/xAPI_logs/JSON_files/prior_statements.pkl'\n",
    "df.to_pickle(converted_file_path)"
   ],
   "id": "b7902e712b6fdf1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "converted_file_path = '/Users/rotelli/PycharmProjects/analyse-de-cours-moodle/src/Datasets/xAPI_logs/JSON_files/prior_statements.pkl'",
   "id": "94d41d5a2f653a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior = pd.read_pickle(converted_file_path)\n",
    "len(prior)"
   ],
   "id": "66424644528c3133",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Big files",
   "id": "13652ef81720ac98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import src.MoodleLogsAlgorithms.data_preprocessing as pp\n",
    "import src.MoodleLogsAlgorithms.function_utils as fu\n",
    "import src.MoodleLogsAlgorithms.data_cleaning as cl\n",
    "import src.MoodleLogsAlgorithms.data_integration as di\n",
    "\n",
    "PATH_JSON = Path(\"../Datasets/xAPI_logs/JSON_files/\")\n",
    "PATH_PICKLE = Path(\"../Datasets/xAPI_logs/PICKLE_files/chunks\")\n",
    "PATH_PREPROC = Path(\"../Datasets/xAPI_logs/PREPROCESSED_files/chunks\")\n",
    "\n",
    "COURSE_1527 = ['1527A', '1527B', '1527C', '1527D', '1527E', '1527F', '1527G']  # S2 OK\n",
    "COURSE_1587 = ['1587A', '1587B', '1587C', '1587D', '1587E', '1587F', '1587G']  # S1\n",
    "COURSE_2781 = ['2781A', '2781B', '2781C', '2781D']  # S2 OK\n",
    "COURSE_3499 = ['3499A', '3499B', '3499C', '3499D', '3499E', '3499F', '3499G', '3499H', '3499I']  # S2\n",
    "\n",
    "COURSES_NAMES = ['Organisation moléculaire du vivant - S2'  # 1527\n",
    "                 'Organisation cellulaire du vivant - S1-23',  # 1587\n",
    "                 'Organisation et fonctions des organismes photosynthétiques - S2-23',  # 2781\n",
    "                 'Mécanique - Physique 2 - S2-23',  # 3499\n",
    "                 ]\n",
    "\n",
    "for course in COURSE_3499:\n",
    "    # ----------------------------\n",
    "    # DATA PREPROCESSING\n",
    "    # ----------------------------\n",
    "    #print('Course: ', course)\n",
    "    #pp.preprocess_json_files(course=course, path_json=PATH_JSON, path_pickle=PATH_PICKLE)\n",
    "    print('Course: ', course)\n",
    "    df = pd.read_pickle(f\"{PATH_PICKLE}/cours_{course}.pkl\")\n",
    "    # fix mistakes\n",
    "    df = fu.patch(df, f\"{PATH_PICKLE}/prior_statements.pkl\")  # TODO: remove on GitHub\n",
    "    # preprocess dataframe data\n",
    "    df = pp.filter_and_rename_columns(df)\n",
    "    df = pp.merge_duplicates_missing_values(df)\n",
    "    df = pp.add_component_event_name(df)\n",
    "    df = pp.extract_ids(df)\n",
    "    df = pp.add_course_area(df)\n",
    "    df = pp.redefine_component(df)\n",
    "    df = pp.redefine_event_name(df)\n",
    "    df = pp.add_activity_status(df)\n",
    "    # fix mistakes\n",
    "    df = fu.patch_modified_names(df)  # TODO: remove on GitHub\n",
    "\n",
    "    # save the dataset for the analysis\n",
    "    df.to_pickle(f\"{PATH_PREPROC}/cours_{course}.pkl\")"
   ],
   "id": "886d65d1ea5f17c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "794418fe01e35de5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\n",
   "id": "c51d07a766513d83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "PATH_PREPROC = Path(\"../Datasets/xAPI_logs/PREPROCESSED_files/chunks\")\n",
    "\n",
    "COURSE_1527 = ['1527A', '1527B', '1527C', '1527D', '1527E', '1527F', '1527G']  # S2\n",
    "COURSE_1587 = ['1587A', '1587B', '1587C', '1587D', '1587E', '1587F', '1587G']  # S1\n",
    "COURSE_2781 = ['2781A', '2781B', '2781C', '2781D']  # S2\n",
    "COURSE_3499 = ['3499A', '3499B', '3499C', '3499D', '3499E', '3499F', '3499G', '3499H', '3499I']  # S2\n",
    "\n",
    "global_course = pd.DataFrame()\n",
    "\n",
    "course_id = 3499\n",
    "\n",
    "for course in COURSE_3499:\n",
    "    print('Course: ', course)\n",
    "    df = pd.read_pickle(f\"{PATH_PREPROC}/cours_{course}.pkl\")\n",
    "    global_course = pd.concat((global_course, df), axis=0)\n",
    "\n",
    "no_duplicate = global_course.drop_duplicates(subset=['User', 'Timestamp', 'Action_verb', 'Object_id', 'Object_type',\n",
    "                                                     'Context', 'Description', 'Path', 'Component',\n",
    "                                                     'Event_name', 'CourseID', 'ObjectID', 'ItemID', 'QuestionID',\n",
    "                                                     'Course_Area', 'Status'])\n",
    "\n",
    "# preprocess user roles\n",
    "df = no_duplicate.copy()\n",
    "role_table = pp.get_role_table(df, course_id)\n",
    "# check for fake students\n",
    "\n",
    "students_to_remove = pp.detect_potential_fake_students(df, course_id)\n",
    "role_table = pp.remove_fake_students(role_table, students_to_remove)\n",
    "\n",
    "# assign roles\n",
    "df = pp.assign_roles(role_table, df, course_id)\n",
    "\n",
    "# ----------------------------\n",
    "# DATA INTEGRATION\n",
    "# ----------------------------\n",
    "# ICAP\n",
    "df = di.integrate_icap_framework(df)\n",
    "# groups\n",
    "group_table = di.get_group_table(df, course_id)\n",
    "df = di.assign_groups(group_table, df, course_id)\n",
    "\n",
    "# ----------------------------\n",
    "# DATA SELECTION\n",
    "# ----------------------------\n",
    "# filter data based on the timestamp\n",
    "df = pp.filter_semester_data(df, course_id)\n",
    "\n",
    "# ----------------------------\n",
    "# DATA CLEANING\n",
    "# ----------------------------\n",
    "# clean the dataset from worthless events\n",
    "df = cl.clean_events(df)\n",
    "\n",
    "# save the dataset for the analysis\n",
    "PATH_PREPROC = Path(\"../Datasets/xAPI_logs/PREPROCESSED_files\")\n",
    "df.to_pickle(f\"{PATH_PREPROC}/cours_{course_id}.pkl\")"
   ],
   "id": "f1d52b9baac7c561",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
